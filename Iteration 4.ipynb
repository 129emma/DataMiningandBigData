{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 4_INFOSYS 722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part starts Data Preparation of Iteration 4\n",
    "# I will number each relevant cell as answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('iteration4').getOrCreate()\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset \n",
    "dataFrame = spark.read.csv('Dataset/data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the dataset\n",
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Exporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computer summary of the data\n",
    "dataFrame.describe().show()\n",
    "\n",
    "# Number of AggrabatedSexualAssault report\n",
    "aggSexAssuaultTable = dataFrame[dataFrame.ANZSOCGroup.isin(\"AggravatedSexualAssault\")]\n",
    "print('\\n' + \"Total of aggravatedSexualAssault reports:\", aggSexAssuaultTable.count())\n",
    "\n",
    "# Number of Non-AggravatedSexualAssault report\n",
    "nonAggravatedSexualAssault = dataFrame[dataFrame.ANZSOCGroup.isin(\"Non-AggravatedSexualAssault\")]\n",
    "print('\\n' + \"Total of Non-AggravatedSexualAssault reports:\", nonAggravatedSexualAssault.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data  Preparation\n",
    "### 3.2 To clean the data, issues must be made explicit, then explicitly resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show orginal columns before cleaning\n",
    "print(\"Total data columns before cleaning:\", dataFrame.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop null values with their rows\n",
    "droppedTable = dataFrame.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns after cleaning\n",
    "print(\"Total data columns after cleaning:\", droppedTable.count())\n",
    "\n",
    "droppedTable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data must be appropriately constructed through the creation of new features/variables, and/or data repositories/tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number, col\n",
    "\n",
    "# Sort table by offenceType\n",
    "groupOffenceType = droppedTable.sort('ANZSOCGroup', ascending = True)\n",
    "\n",
    "# Sort table by Region \n",
    "sortedTable = groupOffenceType.sort('Region',ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas.\n",
    "import pandas as pd\n",
    "\n",
    "# Take the first twenty rows of data, and visualise.\n",
    "pd.DataFrame(sortedTable.take(20), columns=sortedTable.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the data \n",
    "sortedTable.groupby('ANZSOCGroup').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training data\n",
    "sortedTable.printSchema()\n",
    "print(sortedTable.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant packages.\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string indexer which converts every string into a number\n",
    "# such as AggravtedSexAssualt = 0 and Non-AggravtedSexAssualt = 1.\n",
    "# A number will be assigned to every category in the column.\n",
    "# Set offencetype as label\n",
    "offenceType_indexer = StringIndexer(inputCol='ANZSOCGroup',outputCol='label')\n",
    "areaName_indexer = StringIndexer(inputCol='MapDetailName',outputCol='areaIndex')\n",
    "region_indexer = StringIndexer(inputCol='Region',outputCol='regionIndex')\n",
    "occurDay_indexer = StringIndexer(inputCol='OccurrenceDayOfWeek',outputCol='occurDayIndex')\n",
    "weapon_indexer = StringIndexer(inputCol='Weapon',outputCol='weaponIndex')\n",
    "yearMonth_indexer = StringIndexer(inputCol='YearMonth',outputCol='yearMonthIndex')\n",
    "\n",
    "\n",
    "# Conver the various outputs into a single vector.\n",
    "# Multiple columns are collapsed into one. \n",
    "areaName_encoder = OneHotEncoder(inputCol='areaIndex',outputCol='areaVec')\n",
    "region_encoder = OneHotEncoder(inputCol='regionIndex',outputCol='regionVec')\n",
    "occurDay_encoder = OneHotEncoder(inputCol='occurDayIndex',outputCol='occurDayVec')\n",
    "weapon_encoder = OneHotEncoder(inputCol='weaponIndex',outputCol='weaponVec')\n",
    "yearMonth_encoder = OneHotEncoder(inputCol='yearMonthIndex',outputCol='yearMonthVec')\n",
    "\n",
    "\n",
    "# Use vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['areaVec','regionVec','occurDayVec',\n",
    "                                       'weaponVec','yearMonthVec','NumberofVictimisations', \n",
    "                                       'NumberofRecords'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Go through each columns to pipeline\n",
    "pipeline = Pipeline(stages=[areaName_indexer, region_indexer, occurDay_indexer, weapon_indexer, yearMonth_indexer, \n",
    "                            areaName_encoder, region_encoder, occurDay_encoder, weapon_encoder, yearMonth_encoder,\n",
    "                            assembler])\n",
    "\n",
    "# Apply it to the data.\n",
    "pipeline_model = pipeline.fit(sortedTable)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "pipe_df = pipeline_model.transform(sortedTable)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
